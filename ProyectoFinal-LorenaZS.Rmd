---
 date: "2/23/2020"
 output: html_document
---

# Proyecto Final

## Modelo predictivo de la asistencia de pacientes a sus citas

### Elaborado por: Lorena Zúñiga Segura

### Abril 2020

```{r  message =FALSE}
knitr::opts_chunk$set(echo =TRUE)

 # carga de bibliotecas requeridas
 library(lubridate)
 library(ggplot2)
 library(kableExtra)
 library(corrplot)
 library(knitr)
 library(rpart)
 library(rpart.plot)
 library(traineR)
 library(ada)
 library(randomForest)
 library(glmnet)
 library(caret)
 library(ROCR) 

 options(digits = 5)
```

### Funciones auxiliares
```{r}

   # calcular la precisión global dada la matriz de confusión

   calcPrecisionG<-function(matrizC)
   {
       p<- (matrizC[1,1]+ matrizC[1,2])/(matrizC[1,1]+matrizC[1,2]+matrizC[2,1]+matrizC[2,2])
       p<-round(p*100)
       return ( p)
   }

 # calcular la precisión para la clase positiva dada la matriz de confusión

  calcPrecisionPos<-function(mc)
  {
     pp<- mc[2,2]/ (mc[2,1]+mc[2,2])
     return (round(pp * 100))
  }
  
   # calcular la precisión para la clase negativa dada la matriz de confusión

  calcPrecisionNeg<-function(mc)
  {
     pneg<- mc[1,1]/ (mc[1,1]+mc[1,2])
     return (round(pneg * 100))
  }
  
   # calcular las diferentes precisiones como % dada la matriz de confusión

  calcProp.MatConf<-function(mc)
  {
     return( prop.table(mc) * 100)
    
  }
  
   # convertir a factor algunas columnas de los datos utilizados

  toFactor<-function(datos)
  {
    df<-datos
    # se transforma a factor los valores de Gender
  
    df$Gender<-ifelse(df$Gender=='M',1,0)
    df$Gender<-factor(df$Gender)
    
    df$No.show<-ifelse(df$No.show=='No',0,1)
    df$No.show<-factor(df$No.show)
    
    df$Scholarship<-factor(df$Scholarship)
    df$Hipertension<-factor(df$Hipertension)
    df$Diabetes<-factor(df$Diabetes)
    df$Alcoholism<-factor(df$Alcoholism)
    df$Handcap<-factor(df$Handcap)
    df$SMS_received<-factor(df$SMS_received)
    return (df)
  }


  # función para buscar un corte de probabilidad con los datos de testing,  
  # determina cuando la precisión de la clase positiva va subiendo pero sin
  # afectar la precisión de la clase negativa, la cual debe ser de un 85% o superior
  # la función retorna para el modelo elegido:  precisión global, precisión para
  # la clase negativa, precisión para la clase positiva y corte de probabilidad con el que se alcanzaron esos valores de precisión
  
  buscarCorteProb<-function(modelo,datostest)
  {
    precisionNo<-0
    precisionSi<-0
    corteProb<-0
    precisionGlobal<-0
    
    if ((class(modelo)[1])!='prmdt')
    {
      prediccion<- predict(modelo,datostest[,-9],type='prob')
    }
    else
    {
      if (class(modelo)[2] == 'knn.prmdt')
      {
        prediccion<- predict(modelo,datostest,type='prob')
      }
      else
      {
         prediccion<- predict(modelo,datostest[,-9],type='prob')
      }
    }

    clase<- datostest[,9]  #variable a predecir esta en la columna 9
    
    if (class(prediccion) == 'matrix')
    {
      score<-prediccion[,2]
    }
    else
    {
      score<- prediccion$prediction[,2]
    }
    
     
    for (corte in seq(1,0,by=-0.05))
    {
        Prediccion<- ifelse(score>corte,'No.show=1','No.show=0')
        MConf<- table(clase,Pred=factor(Prediccion,levels = c('No.show=0','No.show=1')))
        cat("\nCorte usado para la probabilidad = ")
        cat(corte)
        cat("\n\n")
        cat("Indices\n")
        ind<- general.indexes(mc=MConf)
        print(ind)
        cat("\n==================================")
        
        # detecta cuando la precisión de la clase negativa está entre 0.85 y <1
        
        if ((ind$category.accuracy[[1]]>= 0.80) & (ind$category.accuracy[[1]]<1))
        {
           precisionNo<-ind$category.accuracy[[1]] * 100
           precisionSi<-ind$category.accuracy[[2]] * 100
           corteProb<-corte
           precisionGlobal<-ind$overall.accuracy*100
        }
    }
     
    return (c(precisionNo,precisionSi,corteProb,precisionGlobal) )
  }
  
  
  ## función para crear la curva ROC
  
  crearCurvaROC<-function(predClase,claseReal,corteP,nombre)
  {
    pred<-prediction(predictions = predClase,labels=claseReal)

    perf<- performance(pred,measure='tpr',x.measure = 'fpr')
    curva<-plot(perf,main=paste('Curva ROC para modelo final',nombre),col='blue',lwd=3)
    curva<-curva +abline(a = 0, b = 1, lwd = 2, lty = 2)
    curva
    
    perf.auc<-performance(pred,measure = 'auc')
    auc<- unlist(perf.auc@y.values)
    
    return (auc)
  }
  
  # función para determinar la clase a partir de una probabilidad dada
  
  obtenerClase<-function(pPredicciones,pProba)
  {
    predClase<-c()
    
    vecPredicc<-unlist(pPredicciones)
    cant<-length(vecPredicc)/2
    
    for(i in seq(1:cant))
    {
      predClase<-c(predClase,ifelse(vecPredicc[i+cant]>pProba,1,0))
    }
    return(predClase)
  }
```

# II  Fase de Comprensión de los datos   

## 2.1 Recolección de los datos iniciales

Para la recolección de los datos iniciales asociados a este proyecto no se requirió de permisos o accesos a bases de datos, ya que los mismos fueron obtenidos en kaggle.com, específicamente en la siguiente dirección: <https://www.kaggle.com/joniarroba/noshowappointments/data>  
De esa fuente se obtuvo un único archivo csv que contiene toda la información disponbile al respecto.  


## 2.2 Descripción de los datos

```{r}
# carga de los datos

  datosCitas<-read.csv('KaggleV2-May-2016.csv',header = TRUE,sep = ',',stringsAsFactors = FALSE)

 # se muestran las 10 primeras filas

  kable(head(datosCitas,10)) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped')
```


El conjunto de datos contiene ```r nrow(datosCitas) ``` observaciones y ```r ncol(datosCitas)``` atributos o columnas.  

Cada observación corresponde a una cita programada en el centro médico, entre 2015
(62 registros) y 2016 (110465 registros). 

A continuación se describe el significado de cada uno de los atributos:  

* **PatientId**: Identificador único del paciente.   

* **AppointmentId**: Identificador único de la cita médica.  

* **Gender**: Sexo del paciente.  

* **ScheduledDay**: Fecha en la que se programó la cita.  Se observa que este campo incluye tanto la fecha como la hora.  

* **AppointmentDay**: Fecha de la cita.Se observa que este campo incluye tanto la fecha como la hora.    
* **Age**: Edad del paciente (en años).  

* **Neighbourhood**: Localidad donde será la cita.  

* **Scholarship**: Indicador (tipo sí o no) de si el paciente recibe una ayuda económica del gobierno (de Brasil en este caso).  

* **Hipertension**: Indicador de si el paciente tiene o no hipertensión.  

* **Diabetes**: Indicador de si el paciente tiene o no diabetes.  

* **Alcoholism**: Indicador de si el paciente padece o no de alcoholismo.  

* **Handcap**: Indicador de si el paciente tiene o no alguna discapacidad.  

* **SMS_received**: Indicador de si el paciente recibió o no un mensaje SMS recordándole la cita.  

* **No.show**: Indicador de si el paciente se presentó o no.  Si No.show es igual a Yes, se interpreta como que el paciente no llegó a su cita, inversamente, si No.show es igual a No, se interpreta como que el paciente sí asistió a la cita.  

**De las variables anteriores, la variable a predecir es No.show**    


## 2.3 Exploración de los datos

### 2.3.1 Estadísticas básicas

A continuación se obtienen estadísticas básicas de los datos:  
```{r}
  summary(datosCitas)
```

### 2.3.2 Tipos de datos originales
En lo que se refiere al tipo de datos inicial de cada atributo, se muestra seguidamente:  
```{r}
  str(datosCitas)
```

De los resultados anteriores se observa que:  

* los atributos PatientId y AppointmentId son tipo int.  

* los atributos AppointmentDay y ScheduledDay son tipo caracter. Sin embargo,deberían ser tipo Date.  

* los atributos Hipertension, Diabetes, Alcoholism,Handcap y SMS_received son actualmente numéricos, con valor 1 o 0.  Estos valores son indicadores tipo Yes/No (1/0) respectivamente, según la descripción de los datos previamente dada.  El valor 1 se interpreta como equivalente a Yes o True y el valor 0 como equivalente a No o False.  

* el atributo No.show, que es la variable a predecir, es de tipo caracter, es decir categórico.      


### 2.3.3 Tablas de frecuencia  

Seguidamente se muestran las tablas de frecuencia para explorar los atributos categóricos.  

#### **Tabla de frecuencia y gráfico de barras para el atributo Gender**
```{r}
  
 kable(table(datosCitas$Gender,useNA ='always',dnn=c('Gender'))) %>% kable_styling(position = 'center',full_width = F,fixed_thead = T)

 # tabla usando proporciones
 kable(prop.table(table(datosCitas$Gender,useNA ='always',dnn=c('Gender')))*100) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```
Se tiene que la mayoría de las citas (65%) fueron programadas por pacientes de sexo femenino.  


#### **Tabla de frecuencias para el atributo Neighbourhood**
```{r}
  library(rmarkdown)
    paged_table(as.data.frame(table(datosCitas$Neighbourhood)))
```


#### **Tablas de frecuencias para el atributo Scholarship**
```{r}
  kable(table(datosCitas$Scholarship,useNA ='always',dnn=c('Scholarship')))  %>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
   
 # tabla usando proporciones
 kable(prop.table(table(datosCitas$Scholarship,useNA ='always',dnn=c('Scholarship')))*100) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

Se observa que la gran mayoría de los pacientes (90%) no recibe ayuda del gobierno.  

#### **Tablas de frecuencias para el atributo Hipertension**
```{r}
  kable(table(datosCitas$Hipertension,useNA ='always',dnn=c('Hipertension'))) %>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

 # tabla usando proporciones
 kable(prop.table(table(datosCitas$Hipertension,useNA ='always',dnn=c('Hipertension')))*100) %>% 
   kable_styling(full_width = F,fixed_thead = T)
```

De la tabla anterior se tiene que un 20% de los pacientes padece hipertensión.  

#### **Tablas de frecuencias para el atributo Diabetes**
```{r}
  kable(table(datosCitas$Diabetes,useNA ='always',dnn=c('Diabetes'))) %>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
  
 # tabla usando proporciones
 kable(prop.table(table(datosCitas$Diabetes,useNA ='always',dnn=c('Diabetes')))*100) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla anterior se tiene que un 93% de los pacientes no tienen diabetes, mientras que un 7% sí tiene este padecimiento.  

#### **Tablas de frecuencias para el atributo Alcoholism**
```{r}
  kable(table(datosCitas$Alcoholism,useNA ='always',dnn=c('Alcoholism'))) %>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

 # tabla usando proporciones
 kable(prop.table(table(datosCitas$Alcoholism,useNA ='always',dnn=c('Alcoholism')))*100) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla anterior se tiene que un 97% de los pacientes no presentan alcoholismo, mientras que un 3% sí tiene esta condición.  

#### **Tablas de frecuencias para el atributo Handcap**
```{r}
  kable(table(datosCitas$Handcap,useNA ='always',dnn=c('Handcap'))) %>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
  # tabla usando proporciones
 
  kable(prop.table(table(datosCitas$Handcap,useNA ='always',dnn=c('Handcap')))*100) %>% 
    kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

```

Se observa que existen registros con valores no válidos para este atributo, pues según la descripción de los datos obtenida en la fuente, Handcap es un indicador tipo Verdadero/Falso, por lo tanto los únicos valores válidos son 0 (Falso) y 1 (Verdadero). Sin embargo, se ve que hay valores como 2, 3 y 4, que en este caso no son valores válidos. 

#### **Tablas de frecuencias para el atributo SMS_received**
```{r}
  t<-table(datosCitas$SMS_received,useNA ='always',dnn=c('SMS_received'))
  kable(t) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

 # tabla usando proporciones
 kable(prop.table(t)) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla anterior se tiene que un 68% de los pacientes no recibió un mensaje SMS de recordatorio, mientras que un 32% sí recibió uno o más de estos mensajes.    

#### **Tablas de frecuencias para el atributo No.show**
```{r}
  t<-table(datosCitas$No.show,useNA ='always',dnn=c('No.show'))
  kable(t) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

 # tabla usando proporciones
  
 kable(prop.table(t)*100) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

```
De la tabla anterior se observa que alrededor de un 20% de los pacientes faltaron a su cita (No.Show = Yes), mientras que un 80% sí asistió (No.show = No). Se tiene entonces una clase a predecir desbalanceada.    


#### **Tablas de frecuencia para el atributo ScheduledDay**

Se explora el atributo que corresponde a la fecha en la cual se sacó la cita.  

##### Citas programadas: frecuencia por año
```{r}
 
# se convierten los atributos necesarios a tipo Date

 datosCitas$ScheduledDay <-as.Date(datosCitas$ScheduledDay)

 # exploración de la cantidad de citas programadas por año
 kable(table(year(datosCitas$ScheduledDay),useNA ='always',dnn=c('Year(ScheduledDay)'))) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla se observa que la mayoría de las citas se programaron durante el 2016, mientras que otras fueron agendadas desde el año 2015.  

##### Citas programadas: frecuencia por mes  
```{r}
 # fecha desde cuándo se programó la cita según mes
t<- table(month(datosCitas$ScheduledDay),useNA ='always',dnn=c('Month(ScheduledDay)'))
kable(t) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

kable(prop.table(t)*100)%>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla se tiene que:   
* un 0.05% de las citas se programó desde el mes de enero  
* un 0.25% de las citas se programó desde el mes de febrero  
* un 3.27% de las citas se programó desde el mes de marzo
* casi un 23% de las citas se programó desde el mes de abril  
* un 61% de las citas se programó desde el mes de mayo  
* un 1.25% de las citas se programó desde el mes de junio  
* en los meses de julio hasta octubre inclusive nadie programó citas  o no existen datos al respecto  
* los meses de noviembre y diciembre son los que presentan los menores porcentajes de programación de citas.  


#### **Tablas de frecuencia para el atributo AppointmentDay**

Si bien es cierto la conversión de tipos de datos corresponde a otra fase, en el caso de los atributos AppointmentDay y ScheduledDay, se convertirán a tipo Date en este punto, a fin de facilitar su exploración.  

##### Citas programadas: frecuencia por año
```{r}
 
# se convierten los atributos necesarios a tipo Date

 datosCitas$AppointmentDay <-as.Date(datosCitas$AppointmentDay)

 # exploración de la cantidad de citas programadas por año
 kable(table(year(datosCitas$AppointmentDay),useNA ='always',dnn=c('Year(AppointmentDay'))) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla se observa que todas las citas fueron programadas para el año 2016.  

##### Citas programadas: frecuencia por mes  
```{r}
 # exploración de la cantidad de citas programadas por año
t<- table(month(datosCitas$AppointmentDay),useNA ='always',dnn=c('Month(AppointmentDay'))
kable(t) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

kable(prop.table(t)*100)%>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

De la tabla se tiene que el 24% de las citas se programó para el mes de Junio, un 73% para el mes de Mayo y sólo un 3% fueron programadas para el mes de abril.  

##### Citas programadas: frecuencia por día de la semana  
```{r}
 # exploración de la cantidad de citas programadas por año
t<- table(weekdays(datosCitas$AppointmentDay),useNA ='always',dnn=c('Day of week(AppointmentDay'))
kable(t) %>% kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)

kable(prop.table(t)*100)%>% 
  kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

 Según la tabla anterior, se tiene que un 17% de las citas se programaron para día Viernes, un 21% para el Lunes, un 23% para Martes y en igual porcentaje para el Miércoles. Mientras que un 16% de las citas se programó para el día Jueves y menos del 1% para el día Sábado. 


### 2.3.4 Gráficos de distribución  

### Gráfico de barras para la variable Gender  
```{r fig.align='center', fig.width=4,fig.height=4}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$Gender)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según sexo (gender)', x='Gender',y='Cantidad de citas programadas') 
```

### Gráfico de barras para la variable Neighbourhood  


```{r fig.align='center',fig.width=5,fig.height=9}
# gráfico de barras
  
 ggplot(data=datosCitas,aes(datosCitas$Neighbourhood)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según localidad (neighbourhood)', x='Neighbourhood',y='Cantidad de citas programadas')+coord_flip()
```

### Histograma para la variable edad
```{r fig.align='center', fig.height=5,fig.width=5}
  hist(datosCitas$Age, main='Histograma para Age', xlab = 'Age',ylab='Frecuencia',
       col='steelblue',breaks=5)
```

Del gráfico se observa que:  
* la mayor parte de los pacientes está en el rango entre 20 y 60 años  
* existen edades inferiores a cero  
* existen edades superiores a 100  

### Gráfico de barras para la variable Scholarship  
```{r fig.align='center', fig.width=4,fig.height=4}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$Scholarship)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según Scholarship', x='Scholarship',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que no reciben ayuda del gobierno.  

### Gráfico de barras para la variable Hipertension
```{r fig.align='center', fig.width=4,fig.height=4}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$Hipertension)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según Hipertension', x='Hipertension',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que no padecen de hipertensión.  

### Gráfico de barras para la variable Diabetes
```{r fig.align='center', fig.width=5,fig.height=5}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$Diabetes)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según indicador de Diabetes', x='Diabetes',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que no padecen de diabetes.  

### Gráfico de barras para la variable Alcholism
```{r fig.align='center', fig.width=6,fig.height=4}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$Alcoholism)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según indicador de Alcoholismo', x='Alcholism',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que no padecen alcoholismo.  

### Gráfico de barras para la variable Handcap
```{r fig.align='center', fig.width=6,fig.height=4}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$Handcap)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según indicador de Discapacidad (Handcap)', x='Handcap',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que no padecen discapacidad. Se ve también que hay algunos registros que presentan otros valores (no válidos) para esta variable.  

### Gráfico de barras para la variable SMS_Received
```{r fig.align='center', fig.width=5,fig.height=3}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$SMS_received)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según indicador de SMS_Received', x='SMS Received',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que no padecen discapacidad.   


### Gráfico de barras para la variable a predecir No.show
```{r fig.align='center', fig.width=5,fig.height=3}
 # gráfico de barras
 ggplot(data=datosCitas,aes(datosCitas$No.show)) + geom_bar(fill='steelblue')+
   theme_minimal() + labs(title='Cantidad de citas según indicador de No.show', x='No.Show',y='Cantidad de citas programadas')
```

Se observa del gráfico anterior que la gran mayoría de las citas corresponden a personas que asisten a la cita (No.show= No).  

### Boxplot para las variables Age y No.show
```{r fig.align='center', fig.width=4,fig.height=4}
  boxplot(Age~No.show,data=datosCitas, xlab='Asiste a la cita?',
          ylab='Age', main='Edad de los pacientes y No.show')
```
Del gráfico se observa que en promedio la edad de quienes no asisten a la cita es 40 años, mientras que los que sí asisten tienen en promedio una edad ligeramente inferior.   

## 2.4 Verificación de la calidad de los datos

Para esta actividad ya se han obtenido algunos resultados en la actividad previa, pues se ha observado en las tablas de frecuencia de la mayoría de las variables, que no existen valores nulos mientras que existen problemas de calidad de datos en otras.  
Sin embargo,  se harán nuevamente aquí algunas verificaciones al respecto.  

### 2.4.1 Existencia de valores nulos

```{r}
  # suma de los valores nulos encontrados en los datos

  sum(is.na(datosCitas))
```

Se confirma que no existen valores nulos en los datos.  

### 2.4.2 Existencia de valores no válidos  

Se examinarán las variables Age y Handcap, que ya en la actividad anterior mostraron indicios de problemas de calidad de datos.    

#### Atributo Age
```{r}
 # cantidad de registros con edades inferiore a cero
  negativas<- sum(ifelse(datosCitas$Age<0,1,0))

 # cantidad de registros con edades superiores a 100
  masDeCien<-sum(ifelse(datosCitas$Age>100,1,0))
  
  casos<-negativas+masDeCien
```

En el conjunto de datos existe(n):  
* ```r negativas``` registros con un valor negativo para el atributo Age.  
* ```r masDeCien``` registros con un valor superior a 100 para el atributo Age.

Estos ```r casos ``` casos representan el ```r sprintf('%.2f',(casos/nrow(datosCitas))*100) ``` % del total de registros disponibles.  

#### Atributo Handcap  
Se trata de un atributo tipo Falso/Verdadero, ya que es un indicador de si la persona tiene o no algún tipo de discapacidad. Por lo tanto, los únicos valores válidos serían 1 y 0.  

```{r}
  # conteo de valores de la variable Handcap
  table(datosCitas$Handcap)

  # suma de casos donde la variable Handcap tiene valores distintos a 0 y 1
  noBinario<-sum(ifelse(datosCitas$Handcap>1,1,0))
```
En el conjunto de datos existe(n) ```r noBinario ``` casos con valores no válidos para el atributo Handcap.   
Los registros con valores inválidos representan el ```r sprintf('%.2f',(noBinario/nrow(datosCitas))*100)``` % del total de registros disponibles.  

En ambos casos (atributos Age y Handcap) el total de registros con problemas de calidad de datos representa menos del 1% de la totalidad de observaciones.  

# III Fase de Preparación de los datos   

## 3.1 Selección de los datos

### 3.1.1 Selección de registros u observaciones

Para este proyecto se considerará que edades superiores a 100 años no son válidas.  
En secciones previas se mostró que existen 8 registros con edades no válidas, también hay registros (207) que poseen un código no válido para el atributo Handcap.  

```{r}
  # selección de las posiciones de los registros con valores no válidos en el atributo Age o en el atributo Handcap
  noValidos<-which((datosCitas$Age>100 | datosCitas$Age<0) | (datosCitas$Handcap >1))
```

Esos datos con problemas de calidad representan el ```r (length(noValidos)/nrow(datosCitas))*100 ``` % del total de datos disponibles, por lo que se decide eliminarlos del dataset que se utilizará en fases posteriores del proyecto.  

```{r}
  # se remueven los registros con valores no válidos
  datosCitasCompletos<- datosCitas[-noValidos,]
```

La cantidad de registros finales es de ```r nrow(datosCitasCompletos)```.  

### 3.1.2 Selección de columnas

De los atributos disponibles, descritos en la sección 2.4 Verificación de la calidad de datos , se descartarán los siguientes atributos:  
* PatiendId  
* AppointmentId  

Lo anterior porque son atributos que no aportarán al modelo que se desea plantear, sino que sirven solamente para propósitos de identificar de forma única cada registro dentro del dataset.  


La siguiente figura muestra la matriz de correlación de los atributos numéricos:  

```{r fig.align='center'}

 datosNumCor<- cor(datosCitasCompletos[c('Age','Hipertension','Diabetes','Scholarship','Alcoholism','Handcap','SMS_received')])

corrplot(datosNumCor,order='AOE',addCoef.col = 'black')

```

Del gráfico se observa que la correlación entre algunos atributos es positiva pero baja, a lo sumo 0.5, por ejemplo entre Hipertension y Diabetes, por lo cual todos estos atributos serán conservados.  

Neighbourhood, es otro atributo que no se considerará, principalmente porque no se posee información adicional que permitiera cuantificar la distancia entre el lugar de residencia del paciente y el centro médico. 
 

La fecha en la que el paciente sacó la cita (ScheduledDay) y la fecha de la cita misma (AppointmentDay) tampoco se usarán tal cual están en el dataset, sin embargo sí se derivarán de ellas varios atributos de interés durante la fase de estructuración de los datos, posterior a ello ambas fechas como tales se descartarán.  


```{r}
  # se eliminan las columnas PatiendId, AppointmentId y Neighbourhood
  datosCitasCompletos$PatientId<-NULL
  datosCitasCompletos$AppointmentID<-NULL
  datosCitasCompletos$Neighbourhood<-NULL
```


## 3.2 Limpieza de los datos 

En este caso, dado que fueron eliminados los registros que tenían algunos valores no válidos, no es necesario realizar actividades de limpieza de datos.  

## 3.3 Estructuración de los datos

### 3.3.1 Derivación de atributos  

Se considera de interés para el proyecto conocer lo siguiente:  

*  la cantidad de días existentes entre la fecha en la cual se programa la cita y la fecha de la cita misma. Para esto se creará un nuevo atributo llamado **DaysToApp**, a partir de los atributos ScheduledDay y AppointmentDay.  

*  el día de la semana  para el cual se programó la cita. El atributo se llamará **AppDay**. Este atributo se obtendrá a partir de la fecha de AppointmentDay


```{r}
 
 # se crea el atributo DaysToApp, que básicamente es la diferencia en días entre la fecha en que se hizo la programación de la cita y la fecha de la cita misma, es decir, fecha de la cita - fecha en que se sacó la cita
   
 diasDiferencia<- datosCitasCompletos$AppointmentDay-datosCitasCompletos$ScheduledDay
 datosCitasCompletos$DaysToApp <- as.numeric(diasDiferencia,units='days')
 
 # se crea el atributo AppDay que representa el número de día de la semana de la cita programada
 datosCitasCompletos$AppDay<- wday(datosCitasCompletos$AppointmentDay)
 
# se descartan las fechas como tales
 datosCitasCompletos$ScheduledDay <- NULL
 datosCitasCompletos$AppointmentDay <- NULL
```

### 3.3.2 Generación de registros  

En este caso no se generan nuevos registros.  

## 3.4 Integración de los datos  

### 3.4.1 Unificación de datos  

En este caso, como se indicó en fases previas, se trabaja únicamente con una fuente de datos, por lo que no es requerido una integración de fuentes de datos distintas, ni de tablas de una misma fuente.    

## 3.5 Formateo de los datos

En este caso no se harán conversiones de formato como tal de los datos.  Los atributos que son indicadores se conservarán como datos enteros.  
Por otro lado, las conversiones de tipos que se requerían (tipo Date por ejemplo para los atributos AppointmentDay y ScheduledDay), ya se realizaron para ejecutar actividades propias de fases previas. 

# IV Fase de Modelado    

## 4.1 Selección de la técnica de modelado

Para este caso se utilizarán las siguientes técnicas de modelado:

* Arboles de decisión  
* Bosques aleatorios  
* Boosting
* SVM
* k-Nearest Neighborgs  

### 4.1.1 Técnica seleccionada y supuestos del modelo

A continuación se describe para cada técnica los supuestos de cada modelo:

* **Arboles de decisión**   

Se prefiere que las variables sean categóricas.  

* **Bosques aleatorios**  

Requiere que la variable a predecir sea categórica, los predictores pueden ser numéricos continuos o categóricos, por lo que será necesario convertir a factor la variable Gender (actualmente caracter), así como todos los atributos que representan indicadores. 
Las variables son independientes e idénticamente distribuidas.  

* **Boosting**

* **SVM**  

Requiere que la variable a predecir sea binaria, los predictores pueden ser numéricos continuos o categóricos, por lo que será necesario convertir a factor la variable Gender (actualmente caracter), así como todos los atributos que representan indicadores.  
Las variables son independientes e idénticamente distribuidas.  

 
* **k-Nearest Neighborgs**   
Requiere que todos los datos sean numéricos. 
No requiere distribuciones específicas de los datos.  


## 4.2 Generar el plan de prueba   

### 4.2.1 Plan de pruebas  

En vista de que el problema es de clasificación, se trabajará con un subconjunto de los datos, en este caso una muestra aleatoria de 5000 registros.  

Ese conjunto se dividirá luego en un grupo de datos de entrenamiento y otro de pruebas o testing, en porcentajes de 70% y 30% respectivamente.  


Por otro lado, se espera generar varios modelos con cada técnica:  

* Un primer modelo con cada técnica usando los parámetros por omisión que presentan las funciones de aprendizaje supervisado del paquete predictoR.  

* Otros modelos variando en algunas técnicas todos o algunos de los parámetros por omisión para la técnica. Por ejemplo, los diferentes kerneles en el caso de k-NN.    

#### Selección de la muestra  

```{r}
   
 # se eligen aleatoriamente 5mil filas del conjunto de datos total
  filasMuestra<- sample(1:nrow(datosCitasCompletos),5000)
  datosMuestra<- datosCitasCompletos[filasMuestra,]
   
 # se convierten a factor las variables tipo indicador tanto para la muestra como para todos los datos originales
  
  datosCitasCompletos.final<-toFactor(datosCitasCompletos)
  
  cp.datosMuestra<-toFactor(datosMuestra)
  
 # se divide la muestra en 70% de datos para entrenamiento y 30% para testing/pruebas
   
   filasTesting <- sample(1:nrow(datosMuestra),floor(nrow(datosMuestra)*0.3))
   
   datosTesting<- cp.datosMuestra[filasTesting,]
   
   datosAprend<- cp.datosMuestra[-filasTesting,]
   
   write.csv(datosMuestra,'datosFinal.csv',quote = F,row.names = F)
```


## 4.3 Construcción del modelo   

Como parte de esta tarea se encuentran las actividades de configuración de los parámetros del modelo, la construcción y ejecución del modelo, así como su descripción y evaluación.  

A continuación se muestran las actividas antes citadas para cada una de las técnicas elegidas inicialmente.

### Construcción y ejecución de los modelos - Primera iteración

```{r}
   # se crean vectores globales para almacenar los % de precisión de cada clase en cada modelo, así como resultados de calibración

  modelos<-c('Arboles de decisión','SVM','Bosques Aleatorios','Boosting','k-NN')
  modelos.PrecNo.show.0<-c()
  modelos.PrecNo.show.1<-c()
  modelos.PrecGlobal<-c()
  
  modelosCP.PrecNo.Show.0<-c()
  modelosCP.PrecNo.Show.1<-c()
  modelosCP.PrecGlobal<-c()
  modelos.corteProb<-c()
```

#### **Modelo utilizando Arboles de Decisión**  

##### **Configuración de los parámetros, construcción y descripción del modelo**  

El primer modelo se trabaja con:  

* una profundidad máxima de 15 nodos  
* Entropía como índice de división
* cantidad mínima para hacer un split = 2  

```{r fig.align='center', out.width='60%'}

    # configuración de los parámetros
     modeloAD<- rpart(No.show ~., data = datosAprend,
                   control = rpart.control(minsplit = 2, maxdepth = 15),
                   parms = list(split = 'information'))

    printcp(modeloAD)
    
    rpart.plot(modeloAD)
```

Como se observa el modelo sólo predice la clase No.show=No,  precisamente por el desbalance ya mencionado de la variable a predecir.  

Básicamente el modelo sólo predice cuando el paciente sí asistirá (No.show=No).    


##### **Ejecución del modelo y evaluación**  

```{r}
   
   predicciones<- predict(modeloAD,datosTesting,type='class')

   matConf<- table(datosTesting$No.show,predicciones)
   precisionG<- calcPrecisionG(matConf)
   prop.matConf<- calcProp.MatConf(matConf)
   
   modelos.PrecNo.show.0<-c(modelos.PrecNo.show.0,calcPrecisionNeg(matConf))
   modelos.PrecNo.show.1<-c(modelos.PrecNo.show.1,calcPrecisionPos(matConf))
   modelos.PrecGlobal<-c(modelos.PrecGlobal,precisionG)
   
   prop.matConf 
```

* Se observa que un ```r sprintf('%.2f',prop.matConf[1,1])```%  de las predicciones de No.show=No se hicieron correctamente, mientras que el restante ```r sprintf('%.2f',prop.matConf[2,1])``` % fueron erróneas (falsos negativos).  
* Para No.show=Yes, las predicciones nunca fueron correctas (0%)  
* Precisión global = ```r sprintf('%.2f',precisionG) ```%  
Sin embargo, no se detectó ningún caso para la clase de interés No.show=Yes  

* Precisión positiva = ```r sprintf('%.2f',calcPrecisionPos(matConf))```%  
* Precisión negativa = ```r sprintf('%.2f',calcPrecisionNeg(matConf))```%  

##### **Búsqueda de corte de probabilidad para el modelo:**

Dado que los resultados obtenidos con el modelo de árboles de decisión no son satisfactorios, se procederá a calibrar el modelo de la siguiente forma:

```{r}
   
    # se busca la probabilidad de corte para este modelo
    res<-buscarCorteProb(modeloAD,datosTesting)
    
    # se almacena el corte de probabilidad elegido y las precisiones por clase
    modelosCP.PrecNo.Show.0<-c(modelosCP.PrecNo.Show.0,res[1])
    modelosCP.PrecNo.Show.1<-c(modelosCP.PrecNo.Show.1,res[2])
    modelos.corteProb<-c(modelos.corteProb,res[3])
    modelosCP.PrecGlobal<-c(modelosCP.PrecGlobal,res[4])

```
<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
De los resultados del proceso de búsqueda se tiene que: no se llega a una probabilidad de corte tal que, al subir el % de predicción acertada para la clase de interés (No.show=1), no disminuya mucho el % de aciertos de la otra categoría.  
</div>

#### **Modelo utilizando SVM**  

##### **Configuración de los parámetros, construcción y descripción del modelo** 

El primer modelo con SVM se configura con kernel radial.  
La variable Gender se transforma a numérica 1= M, 0 = F 

```{r}
      modeloSVM<- train.svm(No.show~.,data=datosAprend,kernel='radial',probability=TRUE)
      modeloSVM
```

Se observa que el modelo utiliza ```r modeloSVM$tot.nSV``` vectores de soporte.  

##### **Ejecución del modelo y evaluación**  

```{r}
   
   # se ejecuta el modelo sobre los datos de testing
    predicciones<-predict(modeloSVM,datosTesting[,-9])

  # se obtienen la matriz de confusión y los índices generales
    matConf<-confusion.matrix(datosTesting,predicciones)
    
    matConf 
    
    modelos.PrecNo.show.0<-c(modelos.PrecNo.show.0,calcPrecisionNeg(matConf))
    modelos.PrecNo.show.1<-c(modelos.PrecNo.show.1,calcPrecisionPos(matConf))
    
    indices<-general.indexes(mc=matConf)
    prec<-indices$overall.accuracy*100
    modelos.PrecGlobal<-c(modelos.PrecGlobal,prec)
    
    indices
  
```

Del modelo anterior se tienen los siguientes índices generales:

* La precisión global es de ```r sprintf('%.2f',indices$overall.accuracy * 100)```%  
* La precisión por clase es : ```r sprintf('%.2f',(indices$category.accuracy) * 100)```%


Similar al modelo previo, los resultados muestran una precisión global buena, sin embargo, la precisión por clase muestra nuevamente que cuando se trata de predecir No.show=No (valor 0) la precisión es casi de 100%, mientras que para la clase que interesa predecir No.show=Yes (valor 1) mucho más baja.  

##### **Búsqueda del corte de probabilidad para el modelo:**

Dado que los resultados obtenidos con el modelo con SVM no son satisfactorios, se procederá a calibrar el modelo de la siguiente forma:

```{r}
   
    res<-buscarCorteProb(modeloSVM,datosTesting)

    # se almacena el corte de probabilidad elegido y las precisiones por clase
    modelosCP.PrecNo.Show.0<-c(modelosCP.PrecNo.Show.0,res[1])
    modelosCP.PrecNo.Show.1<-c(modelosCP.PrecNo.Show.1,res[2])
    modelos.corteProb<-c(modelos.corteProb,res[3])
    modelosCP.PrecGlobal<-c(modelosCP.PrecGlobal,res[4])
```
<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
**Del proceso anterior se observa que para ese modelo el mejor corte es con la probabilidad = ```r sprintf('%.2f',res[3])```% , que es cuando logra detectar el mayor % de No.show=1 (clase de interés) y no baja tanto la detección de No.show=0 (no). Aún así, el % detectado es bajo: cercano al ```r sprintf('%.2f',res[2])```%**
</div>

#### **Modelo utilizando Bosques Aleatorios**  

##### **Configuración de los parámetros, construcción y descripción del modelo** 

El primer modelo con Bosques aleatorios se configura con 20 árboles y un total de 3 variables elegidas aleatoriamente para hacer los splits.  

```{r}
      # modelo bosques aleatorios

     modeloBA<-  randomForest(No.show~.,data=datosAprend,importance=TRUE, ntree=20,mtry=3)

     modeloBA
```

##### **Ejecución del modelo y evaluación**  

```{r }
   
   # se ejecuta el modelo sobre los datos de testing
    predicciones<-predict(modeloBA,datosTesting[,-9])

   # se obtienen la matriz de confusión y los índices generales
    
    matConf<-confusionMatrix(datosTesting$No.show,predicciones)
    
    matConf$table 
  
    modelos.PrecNo.show.0<-c(modelos.PrecNo.show.0,calcPrecisionNeg(matConf$table))
    modelos.PrecNo.show.1<-c(modelos.PrecNo.show.1,calcPrecisionPos(matConf$table))
    prec<-matConf$overall[[1]]*100
    modelos.PrecGlobal<-c(modelos.PrecGlobal,prec)
```

Del modelo anterior se tienen los siguientes índices generales:

* La precisión global es de ```r sprintf('%.2f',indices$overall.accuracy * 100)```%  
* La precisión por clase es : ```r sprintf('%.2f',(indices$category.accuracy) * 100)```%


Similar a modelos previos, los resultados muestran una precisión global buena, sin embargo, la precisión por clase muestra nuevamente que cuando se trata de predecir No.show=No (valor 0) la precisión es casi de 100%, mientras que para la clase que interesa predecir No.show=Yes (valor 1) prácticamente cero.  

##### **Buscar corte de probabilidad:**

Dado que los resultados obtenidos con el modelo con SVM no son satisfactorios, se procederá a buscar un corte de probabilidad para el modelo de la siguiente forma:

```{r}
    res<-buscarCorteProb(modeloBA,datosTesting)

    # se almacena el corte de probabilidad elegido y las precisiones por clase
    modelosCP.PrecNo.Show.0<-c(modelosCP.PrecNo.Show.0,res[1])
    modelosCP.PrecNo.Show.1<-c(modelosCP.PrecNo.Show.1,res[2])
    modelos.corteProb<-c(modelos.corteProb,res[3])
    modelosCP.PrecGlobal<-c(modelosCP.PrecGlobal,res[4])
```
<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
**Del proceso anterior se observa que para este modelo el mejor corte es con la probabilidad = ```r res[3]```%, que es cuando logra detectar el mayor % de No.show=1 ```r sprintf('%.2f',res[2])```% y no baja tanto la detección de No.show=0**
</div>

#### **Modelo utilizando Boosting**  

##### **Configuración de los parámetros, construcción y descripción del modelo** 

El primer modelo con Ada boosting se trabajó con los siguientes parámetros:    
* el algoritmo discrete  
* 50 iteraciones  
* profundidad  máxima de 15  
* un mínimo de 20 para dividir los árboles  

```{r }
   
      modeloBoos<- ada(No.show~., data=datosAprend,iter=50,type='discrete',
                       control=rpart.control(minsplit=20,maxdepth=15))

     modeloBoos
```

##### **Ejecución del modelo y evaluación**  

```{r}
 
  # se ejecuta el modelo sobre los datos de testing
    predicciones<-predict(modeloBoos,datosTesting[,-9])

  # se obtienen la matriz de confusión y los índices generales
    matConf<-table(datosTesting$No.show,predicciones)

    matConf 
    
    modelos.PrecNo.show.0<-c(modelos.PrecNo.show.0,calcPrecisionNeg(matConf))
    modelos.PrecNo.show.1<-c(modelos.PrecNo.show.1,calcPrecisionPos(matConf))
    prec<-calcPrecisionG(matConf)
    modelos.PrecGlobal<-c(modelos.PrecGlobal,prec)
```

Se observa nuevamente una precisión del ```r sprintf('%.2f',calcPrecisionNeg(matConf))``` % para No.show=0, pero de apenas ```r sprintf('%.2f',calcPrecisionPos(matConf))```% para la clase de interés en este problema No.show=1.

##### **Búsqueda de corte de probabilidad para el modelo:**

```{r}
   
    res<-buscarCorteProb(modeloBoos,datosTesting)
    
    # se almacena el corte de probabilidad elegido y las precisiones por clase
    modelosCP.PrecNo.Show.0<-c(modelosCP.PrecNo.Show.0,res[1])
    modelosCP.PrecNo.Show.1<-c(modelosCP.PrecNo.Show.1,res[2])
    modelos.corteProb<-c(modelos.corteProb,res[3])
    modelosCP.PrecGlobal<-c(modelosCP.PrecGlobal,res[4])
```

<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
**Del proceso anterior se observa que para ese modelo el mejor corte es con la probabilidad = ```r res[3]```%, que es cuando logra detectar el mayor % de No.show=1 (sí), cerca de un ```r res[2]```% (clase de interés) y no disminuyó drásticamente la detección de Nos.how=0 (no).**  
</div>

### **Modelo utilizando k-NN**  

##### **Configuración de los parámetros, construcción y descripción del modelo** 
 
El primer modelo usando K vecinos más cercanos (k-NN), se configuró escalando los datos, usando el kernel optimal y con un K máximo de 59, tal y como se observa en la siguiente imagen:  
```{r}
   
    modeloKnn <- train.knn(No.show~., data = datosAprend, scale =TRUE, kmax=59, kernel = 'optimal')
  
    modeloKnn
```

Del modelo anterior usando el kernel ```r modeloKnn$best.parameters$kernel```, el mejor k es ```r modeloKnn$best.parameter$k```.  

##### **Ejecución del modelo y evaluación**  

```{r}
 
  # se ejecuta el modelo sobre los datos de testing
    predicciones<-predict(modeloKnn,datosTesting)

  # se obtienen la matriz de confusión y los índices generales
    matConf<-confusion.matrix(datosTesting,predicciones)
    
    matConf 
    
    modelos.PrecNo.show.0<-c(modelos.PrecNo.show.0,calcPrecisionNeg(matConf))
    modelos.PrecNo.show.1<-c(modelos.PrecNo.show.1,calcPrecisionPos(matConf))

    indices<-general.indexes(mc=matConf)
    prec<- indices$overall.accuracy*100
    modelos.PrecGlobal<-c(modelos.PrecGlobal,prec)
    indices
```

Se observa nuevamente una precisión de ```r sprintf('%.2f',calcPrecisionNeg(matConf))```% para No.show=0, pero de ```r sprintf('%.2f',calcPrecisionPos(matConf))``` para la clase de interés en este problema No.show=1.

##### **Búsqueda de corte de probabilidad:**

```{r}
   
   res<-buscarCorteProb(modeloKnn,datosTesting)
   
   # se almacena el corte de probabilidad elegido y las precisiones por clase
   modelosCP.PrecNo.Show.0<-c(modelosCP.PrecNo.Show.0,res[1])
   modelosCP.PrecNo.Show.1<-c(modelosCP.PrecNo.Show.1,res[2])
   modelos.corteProb<-c(modelos.corteProb,res[3])
   modelosCP.PrecGlobal<-c(modelosCP.PrecGlobal,res[4])
```
<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
**Del proceso anterior se observa que para ese modelo el mejor corte es con la probabilidad = ```r sprintf('%.2f',res[3])```%, que es cuando logra detectar el mayor % de No.show=1 (sí), un ```r sprintf('%.2f',res[2])```% (clase de interés)  y al mismo tiempo no disminuye drásticamente la detección de No.show=0 (no).**  
</div>

### **Evaluación general y selección de los mejores modelos**   

#### **Precisión por clase**  

En la siguiente tabla resumen, se muestran los resultados de desempeño obtenidos por los modelos antes descritos después buscar una probabilidad de corte para mejorar la clasificación.   

```{r}
   
 tablaResultados<-data.frame(modelos,modelos.PrecGlobal,modelos.PrecNo.show.0,
                        modelos.PrecNo.show.1,modelos.corteProb,modelosCP.PrecNo.Show.0,
                             modelosCP.PrecNo.Show.1, modelosCP.PrecGlobal)

 colnames(tablaResultados)<- c('Modelos','PrecGlobal','PrecNoShow.0','PrecNoShow.1',
                               'CorteProbabilidad','PrecNoShow.0.CP',
                               'PrecNoShow.1.CP','PrecGlobalCP')
 
 kable(tablaResultados) %>% 
   kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T) 
 
```
<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">

Se observa que las predicciones de la clase de interés fueron mejores con las siguientes  3 las técnicas y modelos :  
*  Bosques aleatorios  
*  Boosting o potenciación  
*  k-NN 

Por lo tanto, se decide ejecutar una segunda iteración variando parámetros para calibrar los modelos utilizando las tres primeras técnicas, es decir, Bosques aleatorios, Boosting y k-NN, a fin de intentar alcanzar una mayor precisión para la clase de interés (No.show=1).  
</div>


### Construcción y ejecución de los modelos - Segunda iteración  

En este paso, se tomarán únicamente las técnicas de Bosques aleatorios, Boosting y k-NN para buscar mejores resultados de clasificación, en lo que a la clase de interés se refiere.  
Para ello se ejecutarán modelos con diferentes parámetros y luego se calibrará el que tenga mejores resultados con cada técnica.  

#### Modelos utilizando Bosques aleatorios

Para esta técnica se ejecutarán varios modelos, variando la cantidad de árboles a crear, parámetro ntree desde 100 hasta 600.  

```{r}
   # se crean vectores globales para almacenar los % de precisión de cada clase en cada modelo, así como resultados de calibración

  modelosBA.PrecNo.show.0<-c()
  modelosBA.PrecNo.show.1<-c()
  modelosBA.PrecGlobal<-c()
```


##### **Configuración de los parámetros, construcción y  ejecución  los modelos**  

```{r}

   numArboles<-seq(100,600,by=100)
   numMtry<-c(1:10) 

   for(numA in numArboles)
   {
     for (m in numMtry)
     {
        #modeloBA<-randomForest(No.show~.,data=datosAprend,importance=TRUE,ntree=numA,mtry=3)

        modeloBA<-randomForest(No.show~.,data=datosAprend,importance=TRUE,ntree=numA,mtry=m)

        modeloBA     
        
        predicciones<-predict(modeloBA,datosTesting)

      # se obtienen la matriz de confusión y los índices generales
    
      matConf<-confusionMatrix(datosTesting$No.show,predicciones)
    
      modelosBA.PrecNo.show.0<-c(modelosBA.PrecNo.show.0,calcPrecisionNeg(matConf$table))
      modelosBA.PrecNo.show.1<-c(modelosBA.PrecNo.show.1,calcPrecisionPos(matConf$table))
      prec<-matConf$overall[[1]]*100
      modelosBA.PrecGlobal<-c(modelosBA.PrecGlobal,prec)
   }

   }   
   resModelosBA<-data.frame(numArboles,numMtry,modelosBA.PrecGlobal,
                            modelosBA.PrecNo.show.0,modelosBA.PrecNo.show.1)
   
   colnames(resModelosBA)<-c('CantArboles','Mtry','PrecGlobal','PrecNo.show.0','PrecNo.show.1')
   
   posMejorBA<-which.max(modelosBA.PrecNo.show.1)
   
   kable(resModelosBA) %>% 
     kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
Como se observa en la tabla anterior, los  mejores resultados para la predicción de la clase de interés (No.Show =1) fueron de ```r sprintf('%.2f',modelosBA.PrecNo.show.1[posMejorBA])```%.  Lo anterior se alcanzó con una cantidad de ```r resModelosBA$CantArboles[posMejorBA]``` árboles y con el parámetro mtry = ```r resModelosBA$Mtry[posMejorBA]```.   
</div>

#### Modelos utilizado k-NN

Para esta técnica se ejecutarán varios modelos, variando los kerneles en cada modelo, a saber: 
* optimal
* rectangular
* triangular
* epanechnikov
* biweight
* triweight
* cos
* inv
* gaussian  

```{r}
   # se crean vectores globales para almacenar los % de precisión de cada clase en cada modelo

  kerneles<-c('optimal','rectangular','triangular','epanechnikov','biweight','triweight','cos','inv','gaussian')
  modelosKNN.PrecNo.show.0<-c()
  modelosKNN.PrecNo.show.1<-c()
  modelosKNN.PrecGlobal<-c()
  mejorK<-c()
```


##### **Configuración de los parámetros, construcción y  ejecución  los modelos**  

```{r}

   for(k in 1:length(kerneles))
   {
      modeloKnn<- train.knn(No.show~., data = datosAprend, scale =TRUE, kmax=70, kernel = kerneles[k])
  
      print(modeloKnn)   
        
      predicciones<-predict(modeloKnn,datosTesting)

      # se obtienen la matriz de confusión y los índices generales
    
      matConf<-confusionMatrix(datosTesting$No.show,predicciones$prediction)
    
      print(matConf)
      
      modelosKNN.PrecNo.show.0<-c(modelosKNN.PrecNo.show.0,calcPrecisionNeg(matConf$table))
      modelosKNN.PrecNo.show.1<-c(modelosKNN.PrecNo.show.1,calcPrecisionPos(matConf$table))
      prec<-matConf$overall[[1]]*100
      modelosKNN.PrecGlobal<-c(modelosKNN.PrecGlobal,prec)
      mejorK<-c(mejorK,modeloKnn$best.parameters$k)
   }

   resModelosKNN<-data.frame(kerneles,mejorK,modelosKNN.PrecGlobal,
                            modelosKNN.PrecNo.show.0,modelosKNN.PrecNo.show.1)
   
   colnames(resModelosKNN)<-c('Kernel','Mejor.K','PrecGlobal','PrecNo.show.0','PrecNo.show.1')
   
   posMejorKNN<-which.max(modelosKNN.PrecNo.show.1)
   
   kable(resModelosKNN) %>% 
     kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
Como se observa en la tabla anterior, de los diferentes kerneles que se probaron, el que generó mejores resultados para la predicción de la clase de interés (No.Show =1) fue el kernel  ```r kerneles[posMejorKNN]```, que logra una precisión de ```r sprintf('%.2f',modelosKNN.PrecNo.show.1[posMejorKNN])```%, con un k= ```r mejorK[posMejorKNN]```.  
</div>


#### Modelos utilizado Boosting

Para esta técnica se ejecutarán varios modelos, variando los algoritmos (real, gentle), así como la cantidad de árboles utilizados (de 100 a 600).  

```{r}
   # se crean vectores globales para almacenar los % de precisión de cada clase en cada modelo

  algoritmos<-c('discrete','real','gentle')
  modelosBoos.PrecNo.show.0<-c()
  modelosBoos.PrecNo.show.1<-c()
  modelosBoos.PrecGlobal<-c()
  numIter<-seq(100,600,by=100)
```


##### **Configuración de los parámetros, construcción y  ejecución  los modelos**  

```{r}
   for(i in 1:length(algoritmos))
   {
     for(j in length(numIter))
     {
       modeloBoos<- ada(No.show~.,data=datosAprend,
                      iter=numIter[j],type=algoritmos[i],
                       control=rpart.control(minsplit=20,maxdepth=15))
  
       modeloBoos  
        
       predicciones<-predict(modeloBoos,datosTesting[,-9])

      # se obtienen la matriz de confusión y los índices generales
       matConf<-table(datosTesting$No.show,predicciones)
       
      matConf 

      modelosBoos.PrecNo.show.0<-c(modelosBoos.PrecNo.show.0,calcPrecisionNeg(matConf))
      modelosBoos.PrecNo.show.1<-c(modelosBoos.PrecNo.show.1,calcPrecisionPos(matConf))
      prec<-calcPrecisionG(matConf)
      modelosBoos.PrecGlobal<-c(modelosBoos.PrecGlobal,prec)
     }
         
           }

    resModelosBoos<-data.frame(c(rep('discrete',6),rep('real',6),rep('gentle',6)),
                               rep(numIter,3),
                               modelosBoos.PrecGlobal,
                               modelosBoos.PrecNo.show.0,modelosBoos.PrecNo.show.1)
   
   colnames(resModelosBoos)<-c('Algoritmo','CantIteraciones','PrecGlobal','PrecNo.show.0','PrecNo.show.1')
   
   posMejorBoos<-which.max(modelosBoos.PrecNo.show.1)
   iterMejorBoos<-numIter[posMejorBoos]
   
   kable(resModelosBoos) %>% 
     kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
```

<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
Como se observa en la tabla anterior, de los diferentes algoritmos que se probaron, el que generó mejores resultados para la predicción de la clase de interés (No.Show =1) fue el algoritmo  ```r algoritmos[posMejorBoos]```, utilizando un total de ```r iterMejorBoos``` iteraciones, que logra una precisión de ```r sprintf('%.2f',modelosBoos.PrecNo.show.1[posMejorBoos])```%.    
</div>


##### **Tercera iteración de los  modelos:**  

Según lo obtenido en la segunda iteración para la configuración y ejecución de modelos, se tomará para cada técnica el modelo que generó mejores resultados en la predicción de la clase de interés, para un total de tres modelos. 
Estos se ejecutan y se comparan, para finalmente elegir el modelo final.


```{r}
 # variables para almacenar resultados de la calibración de cada modelo
 modelos3.PrecNo.Show.0 <- c()
 modelos3.PrecNo.Show.1<-c()
 modelos3.CorteProb<-c()
 modelos3.PrecGlobal<-c()
 nomModelos<-c('Bosques aleatorios','k-NN','Boosting')
```

#### Ejecución modelo de Bosques aleatorios  - tercera iteración

Se tomará el mejor modelo de bosques aleatorios obtenido en la segunda iteración de los modelos.  
Por lo que se configurará con ```r numArboles[posMejorBA]``` árboles.  

```{r fig.align='center', out.width='60%'}
 # se configura y ejecuta el modelo con los mejores parámetros seleccionados en la iteración 2
 modelo.final.BA<- randomForest(No.show~.,data=datosTesting,importance=TRUE,
                                  ntree=resModelosBA$Mtry[posMejorBA],mtry=resModelosBA$Mtry[posMejorBA])

 # se busca un corte de probabilidad para el modelo de bosques aleatorios
 
 res<-buscarCorteProb(modelo.final.BA,datosTesting)
 
 # se almacena el corte de probabilidad elegido y las precisiones por clase
 modelos3.PrecNo.Show.0<-c(modelos3.PrecNo.Show.0,res[1])
 modelos3.PrecNo.Show.1<-c(modelos3.PrecNo.Show.1,res[2])
 modelos3.CorteProb<-c(modelos3.CorteProb,res[3])
 modelos3.PrecGlobal<-c(modelos3.PrecGlobal,res[4])
 
 # se ejecutan las predicciones con el corte de probabilidad encontrado
 
 pred<-predict(modelo.final.BA,datosTesting[,-9],type='prob')

 clasePred<- obtenerClase(pred,res[3])  
 
 # se calcula el AUC y se grafica la curva ROC
 desemp<- crearCurvaROC(clasePred,datosTesting$No.show,res[3],nomModelos[1]) 
  
```

Para el modelo final, utilizando ```r  resModelosBA$CantArboles[posMejorBA]``` árboles y el mejor corte de probabilidad encontrado, es decir, ```r res[3]```,el AUC es de ```r sprintf('%.2f',desemp[1])```.  Sin embargo, se utilizaron aquí los datos de testing.  

#### Ejecución del modelo de k-NN (tercera iteración)

Se tomará el mejor modelo generado con k-NN, el cual fue obtenido en la segunda iteración de los modelos. Por lo que se configurará con  un k de ```r mejorK[posMejorKNN]``` y con el kernel ```r kerneles[posMejorKNN]```.    


```{r fig.align='center',out.width='60%'}
# se configura y ejecuta el modelo con los mejores parámetros seleccionados en la iteración 2

 modelo.final.Knn <- train.knn(No.show~., data = datosAprend, scale =TRUE, 
                               kmax=mejorK[posMejorKNN], kernel = kerneles[posMejorKNN])
 
 # se busca el corte de probabilidad para el modelo usando k-NN
 
 res<-buscarCorteProb(modelo.final.Knn,datosTesting)
 
 # se almacena el corte de probabilidad elegido y las precisiones por clase
 modelos3.PrecNo.Show.0<-c(modelos3.PrecNo.Show.0,res[1])
 modelos3.PrecNo.Show.1<-c(modelos3.PrecNo.Show.1,res[2])
 modelos3.CorteProb<-c(modelos3.CorteProb,res[3])
 modelos3.PrecGlobal<-c(modelos3.PrecGlobal,res[4])

 # se obtienen las predicciones utilizando el  corte de probabilidad encontrado para k-NN
 predicc<-predict(modelo.final.Knn,datosTesting,type='prob')
 clasePred<-obtenerClase(predicc,res[3])
 
 # se calcula el AUC y se grafica la curva ROC
 desemp<- crearCurvaROC(clasePred,datosTesting$No.show,res[3],nomModelos[2]) 

```

Para el modelo final de k-NN, utilizando un k de 
```r  resModelosKNN$Mejor.K[posMejorKNN]```, el kernel ```r resModelosKNN$Kernel[posMejorKNN]``` y el mejor corte de probabilidad encontrado, es decir, ```r res[3]```,el AUC es de ```r sprintf('%.2f',desemp[1])```. 

#### Ejecución del modelo de Boosting (tercera iteración)

Se tomará el mejor modelo de bosques boosting obtenido en la segunda iteración de los modelos.   Por lo que se configurará con  el algoritmo ```r algoritmos[posMejorBoos]``` y con ```r iterMejorBoos``` iteraciones.    

```{r fig.align='center',out.width='60%'}
 # se configura y ejecuta el modelo con los mejores parámetros seleccionados en la iteración 2
 modelo.final.Boos<- ada(No.show~.,      data=datosAprend,iter=iterMejorBoos,type=algoritmos[posMejorBoos],
                       control=rpart.control(minsplit=20,maxdepth=15)) 
 
 res<-buscarCorteProb(modelo.final.Boos,datosTesting)
 
 # se almacena el corte de probabilidad elegido y las precisiones por clase
 modelos3.PrecNo.Show.0<-c(modelos3.PrecNo.Show.0,res[1])
 modelos3.PrecNo.Show.1<-c(modelos3.PrecNo.Show.1,res[2])
 modelos3.CorteProb<-c(modelos3.CorteProb,res[3])
 modelos3.PrecGlobal<-c(modelos3.PrecGlobal,res[4])

 predicc<-predict(modelo.final.Boos,datosTesting[,-9],type='prob')
 
  # se obtienen las predicciones utilizando el  corte de probabilidad encontrado para boosting
 clasePred<-obtenerClase(predicc,res[3])
 
 # se calcula AUC y grafica la curva ROC
 desemp<- crearCurvaROC(clasePred,datosTesting$No.show,res[3],nomModelos[3]) 
 
```
 
 Para el modelo final de Boosting, ya calibrado, utilizando ```r  resModelosBoos$CantIteraciones[posMejorBoos]``` iteraciones, el algoritmo ```r resModelosBoos$Algoritmo[posMejorBoos]``` y el mejor corte de probabilidad encontrado, es decir, ```r res[3]```,el AUC es de ```r sprintf('%.2f',desemp[1])```. 
 
### Evaluación de los modelos finales  

```{r}

   res.final<-data.frame(nomModelos,modelos3.CorteProb,modelos3.PrecGlobal,modelos3.PrecNo.Show.0,modelos3.PrecNo.Show.1)

   colnames(res.final)<-c('Tecnica','CorteProba','PrecGlobal','PrecNoShow.0','PrecNoShow.1')
   
   kable(res.final) %>% 
     kable_styling(full_width = F, position = 'center',bootstrap_options = 'striped',fixed_thead = T)
   
   mejor<-which.max(modelos3.PrecNo.Show.1)
   
```

<style>
div.green { background-color:#B6D7A8; border-radius: 5px; padding: 20px;}
</style>
<div class = "green">
A partir de la tabla anterior se observa que de los tres modelos ya calibrados y con la estimación del corte de probabilidad, el que obtiene mejores resultados es el generado con  ```r nomModelos[mejor]```, con un corte de probabilidad de ```r sprintf('%.2f',modelos3.CorteProb[mejor])```, ya que se obtiene una precisión global de ```r sprintf('%.2f',modelos3.PrecGlobal[mejor])```% y especialmente una precisión de ```r  sprintf('%.2f',modelos3.PrecNo.Show.1[mejor])```% para la clase de interés, a la vez que la precisión para la otra clase (la mayoritaria) es de ```r sprintf('%.2f',modelos3.PrecNo.Show.0[mejor])```%.  
</div>

Sin embargo, para tomar la decisión final se ejecutará una validación cruzada con 10 folds, aplicando los tres modelos anteriores, para lo cual los tres modelos se probarán utilizando todos los datos, no sólo los de testing.    

#### Validación cruzada de los tres modelos, usando 10 folds

```{r}
       # se ejecuta una validación cruzada 1 vez, con 10 folds o grupos 

       erroresBA<-c()
       aciertosBA<-c()
       pGlobalBA<-c()
       pPositivaBA<-c()
       pNegativaBA<-c()
       
       erroresKnn<-c()
       aciertosKnn<-c()
       pGlobalKnn<-c()
       pPositivaKnn<-c()
       pNegativaKnn<-c()
       
       erroresBoos<-c()
       aciertosBoos<-c()
       pGlobalBoos<-c()
       pPositivaBoos<-c()
       pNegativaBoos<-c()
       numFolds<-10
       
       # corte de probabilidad previamente encontrado para el mejor modelo
       
        corteBA<-modelos.corteProb[3]
        corteBoos<-modelos.corteProb[4]
        corteKnn<-modelos.corteProb[5]
        
        
      parcial<-sample(1:nrow(datosCitasCompletos.final),70000)
      datosParcial<-datosCitasCompletos.final[parcial,]
      
      # se crean los 10 grupos 
      #folds<- createFolds(cp.datosMuestra$No.show,k=numFolds)
       folds<-createFolds(datosParcial$No.show,k=numFolds)
      # ciclo para ejecutar la validación cruzada  
      for(g in 1:numFolds) 
      {
        muestra<-folds[[g]]
        datTest<- datosParcial[muestra,]         #cp.datosMuestra[muestra,]
        datAprend<- datosParcial[-muestra,]   #cp.datosMuestra[-muestra,]
        predClase<-c()
        
        # se ejecuta el modelo con los mejores parámetros encontrados 
        
        modeloBA<-randomForest(No.show~.,data=datAprend,importance=TRUE,
                               ntree=resModelosBA$CantArboles[posMejorBA],
                               mtry=resModelosBA$Mtry[posMejorBA])
       
         # se obtienen las predicciones como probabilidades
        
        dpredic<-predict(modeloBA,datTest[,-9], type='prob')
       # según el corte de probabilidad encontrado se define la clase a la que pertenece cada fila
        predClase<-obtenerClase(dpredic,corteBA)
        
        mc<-table(datTest$No.show,predClase)
        ac<-sum(diag(mc))/sum(mc)
        
        aciertosBA<-c(aciertosBA,ac)
        erroresBA<-c(erroresBA,1-ac)
        pGlobalBA<-c(pGlobalBA, calcPrecisionG(mc))
        pPositivaBA<- c(pPositivaBA, calcPrecisionPos(mc))
        pNegativaBA<- c(pNegativaBA, calcPrecisionNeg(mc))
         
        
        modeloKnn<- train.knn(No.show~., data = datAprend, scale =TRUE,
                               kmax=mejorK[posMejorKNN], kernel = kerneles[posMejorKNN])

        dpredic<-predict(modeloKnn,datTest, type='prob')
       # según el corte de probabilidad encontrado se define la clase a la que pertenece cada fila
        predClase<-obtenerClase(dpredic,corteKnn)

        mc<-table(datTest$No.show,predClase)
        ac<-sum(diag(mc))/sum(mc)

        aciertosKnn<-c(aciertosKnn,ac)
        erroresKnn<-c(erroresKnn,1-ac)
        pGlobalKnn<-c(pGlobalKnn, calcPrecisionG(mc))
        pPositivaKnn<- c(pPositivaKnn, calcPrecisionPos(mc))
        pNegativaKnn<- c(pNegativaKnn, calcPrecisionNeg(mc))

        # boosting
        modeloBoos<- ada(No.show~.,      data=datAprend,iter=iterMejorBoos,type=algoritmos[posMejorBoos],
                       control=rpart.control(minsplit=20,maxdepth=15))

         # se obtienen las predicciones como probabilidades

        dpredic<-predict(modeloBoos,datTest[,-9], type='prob')
       # según el corte de probabilidad encontrado se define la clase a la que pertenece cada fila
        predClase<-obtenerClase(dpredic,corteBoos)

        mc<-table(datTest$No.show,predClase)
        ac<-sum(diag(mc))/sum(mc)

        aciertosBoos<-c(aciertosBoos,ac)
        erroresBoos<-c(erroresBoos,1-ac)
        pGlobalBoos<-c(pGlobalBoos, calcPrecisionG(mc))
        pPositivaBoos<- c(pPositivaBoos, calcPrecisionPos(mc))
        pNegativaBoos<- c(pNegativaBoos, calcPrecisionNeg(mc))
       }
      
```

##### Modelo Bosques aleatorios: Tabla resumen de los resultados de la validación cruzada

```{r}
      resultadosBA<-data.frame(erroresBA,aciertosBA,pPositivaBA,pNegativaBA,pGlobalBA) 
      colnames(resultadosBA)<-c('Error','Aciertos','Pos','Neg','Global')
      kable(resultadosBA) %>% kable_styling(full_width = F)
      
      # promedios para bosques aleatorios
       promBA<- c(mean(erroresBA),mean(aciertosBA),mean(pPositivaBA),mean(pNegativaBA),mean(pGlobalBA))

```
Se observa que en promedio, durante la validación cruzada, para el modelo de bosques aleatorios, los resultados fueron:  
* **Promedio precisión clase positiva:**  ```r  sprintf('%.2f',mean(pPositivaBA))```%  
* **Promedio precisión clase negativa:**  ```r  sprintf('%.2f',mean(pNegativaBA))```%  
* **Promedio precisión global:**  ```r  sprintf('%.2f',mean(pGlobalBA))```%    


    
##### Modelo k-NN: Tabla resumen de los resultados de la validación cruzada
    
```{r}      
      resultadosKnn<-data.frame(erroresKnn,aciertosKnn,pPositivaKnn,pNegativaKnn,pGlobalKnn) 
      colnames(resultadosKnn)<-c('Error','Aciertos','Pos','Neg','Global')
      kable(resultadosKnn) %>% kable_styling(full_width = F)
      
      # promedios obtenidos con k-NN
      promKnn<-c(mean(erroresKnn),mean(aciertosKnn),mean(pPositivaKnn),mean(pNegativaKnn),
                 mean(pGlobalKnn))
      
```

Se observa que en promedio, durante la validación cruzada, para el modelo con k-NN  los resultados fueron:  
* **Promedio precisión clase positiva:**  ```r  sprintf('%.2f',mean(pPositivaKnn))```%  
* **Promedio precisión clase negativa:**  ```r  sprintf('%.2f',mean(pNegativaKnn))```%  
* **Promedio precisión global:**  ```r  sprintf('%.2f',mean(pGlobalKnn))```%    

##### Modelo Boosting: Tabla resumen de los resultados de la validación cruzada

```{r}     
      resultadosBoos<-data.frame(erroresBoos,aciertosBoos,pPositivaBoos,pNegativaBoos,
                                 pGlobalBoos) 
      colnames(resultadosBoos)<-c('Error','Aciertos','Pos','Neg','Global')
      kable(resultadosKnn) %>% kable_styling(full_width = F)
      
      #promedios obtenidos con boosting
      promBoos<-c(mean(erroresBoos),mean(aciertosBoos),mean(pPositivaBoos),mean(pNegativaBoos),
                  mean(pGlobalBoos))

```

Se observa que en promedio, durante la validación cruzada, para el modelo usando Boosting  los resultados fueron:  
* **Promedio precisión clase positiva:**  ```r  sprintf('%.2f',mean(pPositivaBoos))```%  
* **Promedio precisión clase negativa:**  ```r  sprintf('%.2f',mean(pNegativaBoos))```%  
* **Promedio precisión global:**  ```r  sprintf('%.2f',mean(pGlobalBoos))```%    

##### Promedios obtenidos con cada modelo. Tabla resumen

```{r}         
       
      tabla.Resumen.VC<- data.frame(c('Bosques Aleatorios','k-NN','Boosting'),
                                    c(promBA[1],promKnn[1],promBoos[1]),
                                    c(promBA[2],promKnn[2],promBoos[2]),
                                    c(promBA[3],promKnn[3],promBoos[3]),
                                    c(promBA[4],promKnn[4],promBoos[4]),
                                    c(promBA[5],promKnn[5],promBoos[5]))
      colnames(tabla.Resumen.VC)<-c('Tecnica','Prom.Error','Prom.Aciertos','Prom.Pos','Prom.Neg','Prom.Global')
      
      kable(tabla.Resumen.VC) %>% kable_styling(full_width = F)
      
```

De la tabla anterior se observa que:  
* los tres modelos obtienen prácticamente la misma precisión global.  
* los tres modelos consiguen mejorar sus porcentajes de precisión para la clase positiva, comparados con los resultados iniciales.  
* el modelo de bosques aleatorios es el que consigue ligeramente mejores resultados para la clase positiva, aunque sigue siendo una precisión baja.  

En vista de lo anterior, el modelo final seleccionado es el de Bosques aleatorios.  


#### Importancia de las variables

Se determinan las variables más importantes para el modelo seleccionado:  

```{r fig.align='center'}
   
   # importancia de las variables
   
   varImpPlot(modelo.final.BA,sort=TRUE,n.var=6)
```

Se observa que las variables DaysToApp (cantidad de días entre la fecha en que se saca la cita y la cita misma)y el haber recibido o no recordatorios (SMS_received) son variables que para el modelo seleccionado tienen importancia.  

# V Fase de Evaluación    

## 5.1 Evaluación de los resultados

### 5.1.1 Valoración de los resultados

Tal y como se determinó en la Fase de entendimiento del negocio, en este caso, los centros médicos que reciben a los pacientes, tienen los siguientes objetivos:  
 
* Identificar a los pacientes que tienen alto riesgo de no presentarse a su cita  
* Reducir la inasistencia de los pacientes a sus citas  
* Determinar si existen aspectos que influyen en el no-show de los pacientes  

Con respecto a los dos primeros objetivos, se considera que el modelo que se construyó finalmente constituye una base que posteriormente podría ayudar a cumplir con dichos objetivos de negocio.  
En el caso del tercer objetivo, se identificaron al menos tres variables, que podrían utilizarse como criterios, para estar más al tanto de los pacientes y darles un seguimiento que culmine con la asistencia a la cita programada.  

Por otro lado, en lo referente a los objetivos de minería de datos, este se cumple en cuanto a la precisión global alcanzada (cerca del 80%), cuando el objetivo buscaba un 70%.  Sin embargo, al no contar con datos suficientes o más equilibrados es posible que el modelo no genere resultados satisfactorios para predecir cuáles pacientes no asisten a sus citas.   

### 5.1.2 Modelos aprobados  

El modelo aprobado es el que se calibró al final, utilizando la técnica de Bosques aleatorios.  

## 5.2 Revisión del proceso

### 5.2.1  Revisión del proceso 

Se considera que se llevaron a cabo las fases sugeridas por la metodología CRISP-DM, incluyendo sus respectivas actividades. Sin embargo, habría sido deseable contar con más tiempo, a fin de probar con otras técnicas como redes neuronales y XGBoosting, que en el caso de la primera sí se intentó, pero no fue posible llegar a que el modelo convergiera, aún cambiando varios de los parámetros.  


## 5.3 Próximos pasos  

Aunque los resultados obtenidos no son satisfactorios, el personal de la clínica considera que el modelo podría ser de utilidad, ya que actualmente no se tiene nada que permita hacer una estimación sobre el No.show de los pacientes.  Además, se logró dos variables o aspectos que podrían influir en que el paciente no se presente, ante lo cual se dará seguimiento a esas variables por parte del personal encargado. 
Se sugiere entonces:
*  continuar hacia la siguiente fase, que implica la implantación del modelo aceptado, a fin de valorar en un futuro si con las predicciones del modelo, mejora la situación de la clínica en lo que a no show de los pacientes se refiere.   
*  valorar la posibilidad de recopilar datos para todo un año completo y volver a ejecutar el modelo.  
 

